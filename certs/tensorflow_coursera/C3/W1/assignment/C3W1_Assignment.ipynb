{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "juvenile-enforcement",
   "metadata": {},
   "source": [
    "# Week 1: Explore the BBC News archive\n",
    "\n",
    "Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-brooklyn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrZevCPJ92HG",
    "outputId": "4ba10d10-1433-42fc-dc8e-83b297705cfe",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-power",
   "metadata": {},
   "source": [
    "Begin by looking at the structure of the csv that contains the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-panic",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "with open(\"./data/bbc-text.csv\", 'r') as csvfile:\n",
    "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-calvin",
   "metadata": {},
   "source": [
    "As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-credit",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n",
    "\n",
    "Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided. You only need to account for whitespace as the separation mechanism between words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-privilege",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: remove_stopwords\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "    \n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "    \n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "    \n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    ### START CODE HERE\n",
    "\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Remove the stopwords\n",
    "    sentence = [word for word in words if word not in stopwords]\n",
    "\n",
    "    # Join the sentence back together\n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "    ### END CODE HERE\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-third",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Test your function\n",
    "remove_stopwords(\"I am about to go to the store and get any snack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-excellence",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "'go store get snack'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-photography",
   "metadata": {},
   "source": [
    "## Reading the raw data\n",
    "\n",
    "Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n",
    "\n",
    "A couple of things to note:\n",
    "- You should omit the first line as it contains the headers and not data points.\n",
    "- There is no need to save the data points as numpy arrays, regular lists is fine.\n",
    "- To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n",
    "- `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n",
    "- Use the `remove_stopwords` function in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "monthly-beginning",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: parse_data_from_file\n",
    "def parse_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        filename (string): path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    # stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        ### START CODE HERE\n",
    "\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            labels.append(row[0])\n",
    "            sentences.append(remove_stopwords(row[1]))       \n",
    "        \n",
    "        # append the first element of the row to the `labels` list (1 line of code)\n",
    "        None\n",
    "        # use the remove_stopwords() function on the second element of the \n",
    "        # row then append to the `sentences` list (1 to 3 lines of code)\n",
    "        None\n",
    "        \n",
    "        ### END CODE HERE\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listed-saturn",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/bbc-text.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test your function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# With original dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m \u001b[43mparse_data_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/bbc-text.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGINAL DATASET:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sentences in the dataset.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mparse_data_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m### START CODE HERE\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(csvfile, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/bbc-text.csv'"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "# With original dataset\n",
    "sentences, labels = parse_data_from_file(\"./data/bbc-text.csv\")\n",
    "\n",
    "print(\"ORIGINAL DATASET:\\n\")\n",
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\\n\\n\")\n",
    "\n",
    "# With a miniature version of the dataset that contains only first 5 rows\n",
    "mini_sentences, mini_labels = parse_data_from_file(\"./data/bbc-text-minimal.csv\")\n",
    "\n",
    "print(\"MINIATURE DATASET:\\n\")\n",
    "print(f\"There are {len(mini_sentences)} sentences in the miniature dataset.\\n\")\n",
    "print(f\"First sentence has {len(mini_sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(mini_labels)} labels in the miniature dataset.\\n\")\n",
    "print(f\"The first 5 labels are {mini_labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-shanghai",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "ORIGINAL DATASET:\n",
    "\n",
    "There are 2225 sentences in the dataset.\n",
    "\n",
    "First sentence has 436 words (after removing stopwords).\n",
    "\n",
    "There are 2225 labels in the dataset.\n",
    "\n",
    "The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n",
    "\n",
    "\n",
    "MINIATURE DATASET:\n",
    "\n",
    "There are 5 sentences in the miniature dataset.\n",
    "\n",
    "First sentence has 436 words (after removing stopwords).\n",
    "\n",
    "There are 5 labels in the miniature dataset.\n",
    "\n",
    "The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-illness",
   "metadata": {},
   "source": [
    "_**Note: The test above should take less than 5 seconds to run. If your implementation runs longer than that, the grader might time out when you submit. Try to improve it and if it still takes a while to execute, you can click the text below for some hints.**_\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><font size=\"3\" color=\"darkgreen\"><b><u>Click for hints</u></b></font></summary>\n",
    "    \n",
    "    # Read the CSV file and specify the comma character as the delimiter\n",
    "    reader = csv.reader(None, delimiter=None)\n",
    "        \n",
    "        # skip the first line using the next() function (1 line of code)\n",
    "        None\n",
    "    \n",
    "        # start for loop and iterate over each row of the reader\n",
    "        for row in reader:\n",
    "    \n",
    "            # append the first element of the row to the `labels` list (1 line of code)\n",
    "            None\n",
    "    \n",
    "            # use the remove_stopwords() function on the second element of the \n",
    "            # row then append to the `sentences` list (1 to 3 lines of code)\n",
    "            None\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-basket",
   "metadata": {},
   "source": [
    "## Using the Tokenizer\n",
    "\n",
    "Now it is time to tokenize the sentences of the dataset. \n",
    "\n",
    "Complete the `fit_tokenizer` below. \n",
    "\n",
    "This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-virginia",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: fit_tokenizer\n",
    "def fit_tokenizer(sentences):\n",
    "    \"\"\"\n",
    "    Instantiates the Tokenizer class\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): lower-cased sentences without stopwords\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    # Instantiate the Tokenizer class by passing in the oov_token argument\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "    # Fit on the sentences\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-hostel",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = fit_tokenizer(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-pollution",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "Vocabulary contains 29714 words\n",
    "\n",
    "<OOV> token included in vocabulary\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-flash",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_padded_sequences\n",
    "def get_padded_sequences(tokenizer, sentences):\n",
    "    \"\"\"\n",
    "    Generates an array of token sequences and pads them to the same length\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
    "        sentences (list of string): list of sentences to tokenize and pad\n",
    "    \n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    # Convert sentences to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad the sequences using the post padding strategy\n",
    "    padded_sequences = pad_sequences(sequences, padding='post')\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-entrepreneur",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
    "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
    "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
    "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-brief",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "First padded sequence looks like this: \n",
    "\n",
    "[  96  176 1157 ...    0    0    0]\n",
    "\n",
    "Numpy array of all sequences has shape: (2225, 2438)\n",
    "\n",
    "This means there are 2225 sequences in total and each one has a size of 2438\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-optimization",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: tokenize_labels\n",
    "def tokenize_labels(labels):\n",
    "    \"\"\"\n",
    "    Tokenizes the labels\n",
    "    \n",
    "    Args:\n",
    "        labels (list of string): labels to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Instantiate the Tokenizer class\n",
    "    # No need to pass additional arguments since you will be tokenizing the labels\n",
    "    label_tokenizer = Tokenizer()\n",
    "    \n",
    "    # Fit the tokenizer to the labels\n",
    "    label_tokenizer.fit_on_texts(labels)\n",
    "        \n",
    "    # Save the word index\n",
    "    label_word_index = label_tokenizer.word_index\n",
    "    \n",
    "    # Save the sequences\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return label_sequences, label_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-conviction",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "label_sequences, label_word_index = tokenize_labels(labels)\n",
    "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
    "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-poultry",
   "metadata": {},
   "source": [
    "***Expected Output:***\n",
    "```\n",
    "Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
    "\n",
    "First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-chess",
   "metadata": {
    "id": "6rITvNKqT-51"
   },
   "source": [
    "**Congratulations on finishing this week's assignment!**\n",
    "\n",
    "You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "dlai_version": "1.2.0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
